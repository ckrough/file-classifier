services:
  file-classifier:
    build:
      context: .
      dockerfile: Dockerfile
    image: file-classifier:latest
    container_name: file-classifier

    # Run as current user to avoid permission issues with created/modified files
    # The container will use your UID:GID, ensuring files are owned by you
    user: "${UID:-1000}:${GID:-1000}"

    # Resource limits (prevents unbounded memory/CPU usage during AI processing)
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 512M

    # Volume mounts
    # You can customize these paths using environment variables:
    #   INPUT_DIR=./my-documents docker-compose run --rm file-classifier /app/input
    #   OUTPUT_DIR=./my-output docker-compose run --rm file-classifier /app/input --move --destination /app/output
    volumes:
      # Mount directory containing files to classify
      # Default: ./files, override with INPUT_DIR environment variable
      - ${INPUT_DIR:-./files}:/app/input:rw

      # Mount directory for output files (if using --move flag)
      # Default: ./output, override with OUTPUT_DIR environment variable
      - ${OUTPUT_DIR:-./output}:/app/output:rw

      # Mount .env file for configuration
      - ./.env:/app/.env:ro
      # Optional: Mount persistent log directory
      # Uncomment to preserve logs across container restarts
      # - ${LOG_DIR:-./logs}:/tmp:rw

      # Environment variables (alternative to .env file mount)
      # Uncomment and configure if not using .env file
      # environment:
      #   - AI_PROVIDER=openai
      #   - OPENAI_API_KEY=your-api-key-here
      #   - AI_MODEL=gpt-4o-mini
      #   - DEBUG_MODE=false

      # Command to run (override this when running)
      # Examples:
      #   docker-compose run --rm file-classifier /app/input/sample.pdf
      #   docker-compose run --rm file-classifier /app/input --dry-run
      #   docker-compose run --rm file-classifier /app/input --move --destination /app/output
      #   INPUT_DIR=./sample-documents docker-compose run --rm file-classifier /app/input --dry-run
    command: [ "--help" ]
    # Network configuration (required if using Ollama service)
    # networks:
    #   - classifier-network

    # Optional: Ollama service for local LLM models
    # Uncomment this section if using AI_PROVIDER=ollama
    # ollama:
    #   image: ollama/ollama:latest
    #   container_name: ollama
    #   ports:
    #     - "11434:11434"
    #   volumes:
    #     - ollama-data:/root/.ollama
    #   networks:
    #     - classifier-network
    #   # For GPU support (NVIDIA), uncomment:
    #   # deploy:
    #   #   resources:
    #   #     reservations:
    #   #       devices:
    #   #         - driver: nvidia
    #   #           count: 1
    #   #           capabilities: [gpu]

    # networks:
    #   classifier-network:
    #     driver: bridge

    # volumes:
    #   ollama-data:
