# =============================================================================
# AI File Classifier - Environment Configuration
# =============================================================================
# Copy this file to .env and configure for your environment
# Usage: cp .env.example .env

# =============================================================================
# General Configuration
# =============================================================================

# Enable debug logging (true/false)
# Set to "true" for detailed technical logs, "false" for normal operation
DEBUG_MODE=false

# =============================================================================
# AI Provider Configuration
# =============================================================================

# Choose your AI provider: "openai" or "ollama"
# - openai: Use OpenAI's cloud API (requires API key, costs per request)
# - ollama: Use local models via Ollama (free, requires local Ollama installation)
AI_PROVIDER=openai

# =============================================================================
# OpenAI Configuration (when AI_PROVIDER=openai)
# =============================================================================

# Your OpenAI API key (required for OpenAI provider)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI model to use (default: gpt-4o-mini)
# Options: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Note: gpt-4o-mini is recommended for cost-effectiveness
AI_MODEL=gpt-4o-mini

# =============================================================================
# Ollama Configuration (when AI_PROVIDER=ollama)
# =============================================================================

# Ollama server URL (default: http://localhost:11434)
# If running Ollama in Docker, use: http://ollama:11434
# If running Ollama on host machine, use: http://host.docker.internal:11434 (macOS/Windows)
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama model name (default: deepseek-r1:latest)
# First pull the model: ollama pull deepseek-r1:latest
# Other options: llama2, mistral, mixtral, etc.
# OLLAMA_MODEL=deepseek-r1:latest

# =============================================================================
# Example Configurations
# =============================================================================

# Example 1: OpenAI Cloud (recommended for simplicity)
# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxx
# AI_MODEL=gpt-4o-mini
# DEBUG_MODE=false

# Example 2: Local Ollama (free, requires local installation)
# AI_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=deepseek-r1:latest
# DEBUG_MODE=false

# Example 3: Ollama in Docker Compose (multi-container setup)
# AI_PROVIDER=ollama
# OLLAMA_BASE_URL=http://ollama:11434
# OLLAMA_MODEL=deepseek-r1:latest
# DEBUG_MODE=false

# Content Extraction Performance Tuning
# These settings control how much document content is extracted and sent to AI
# Optimizing extraction can reduce API costs by 60-80% and improve response times

# Extraction strategy: "full", "first_n_pages", "char_limit", or "adaptive" (recommended)
CLASSIFICATION_STRATEGY=adaptive

# Maximum number of pages to extract (for page-based strategies)
CLASSIFICATION_MAX_PAGES=3

# Maximum characters to extract (safety net to prevent excessive token usage)
# ~10,000 characters â‰ˆ 2,500 tokens
CLASSIFICATION_MAX_CHARS=10000

# Include last page when using page-based extraction
# Useful for documents with summaries/totals on the last page
CLASSIFICATION_INCLUDE_LAST_PAGE=true